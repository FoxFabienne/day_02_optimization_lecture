% no notes
\documentclass{beamer}
% notes and slides
%\documentclass[notes]{beamer}
% notes only
% \documentclass[notes=only]{beamer}
\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{multirow}
\usepackage{multimedia}
\usepackage{url}
\usepackage{standalone}
\usepackage{adjustbox}
\usepackage{lmodern}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{multimedia}
\usepackage{media9}

% plotting.
\usepackage{standalone}
\usepackage{tikz}
\usepackage{pgfplots}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}

\usepackage{csquotes}


\PassOptionsToPackage{american}{babel} % change this to your language(s), main language last
% Spanish languages need extra options in order to work with this template
% \PassOptionsToPackage{spanish,es-lcroman}{babel}
\usepackage{babel}

\PassOptionsToPackage{%
  backend=biber,bibencoding=utf8, %instead of bibtex
  %backend=bibtex8,bibencoding=ascii,%
  language=auto,%
  style=numeric-comp,%
  %style=authoryear-comp, % Author 1999, 2010
  %bibstyle=authoryear,dashed=false, % dashed: substitute rep. author with ---
  style=alphabetic,
  sorting=nyt, % name, year, title
  maxbibnames=10, % default: 3, et al.
  %backref=true,%
  %natbib=true % natbib compatibility mode (\citep and \citet still work)
}{biblatex}
\usepackage{biblatex}

\addbibresource{bib.bib}

\usetheme{metropolis}           % Use metropolis theme
\setbeamertemplate{caption}[default]
\title{Foundations of Machine Learning in Python}
\institute{High Performance Computing and Analytics Lab, Uni Bonn}
\author{Moritz Wolter}
\date{\today}

\titlegraphic{\includegraphics[width=2.00cm]{UNI_Bonn_Logo_Standard_RZ.pdf}}
\begin{document}
    \maketitle

    \begin{frame}
    \frametitle{Overview} 
    \tableofcontents

    \note{TODO}
    \end{frame}


    \begin{frame}{Optimization}
      Traditionally, optimization means minimizing using a cost function $f(x)$.
      Given the cost, we must find the cheapest point $x^*$ on the function,
      or in other words,
      \begin{align}
       x^* = \min_{x \in \mathbb{R}} f(x)        
      \end{align}
    \end{frame}

    \begin{frame}{Functions}
      Functions are mathematical mappings. Consider for example the quadratic funtion,
      $f(x): \mathbb{R} \rightarrow \mathbb{R}$:
      \begin{align}
        f(x) = x^2
      \end{align}

    \begin{figure}
      \includestandalone[scale=.7]{./figures/parabola}
    \end{figure}
    \end{frame}

    \begin{frame}{Where is the minimum?}
      \begin{figure}
        \includestandalone[scale=.7]{./figures/parabola}
      \end{figure}
      In this case we immediately see it's at zero. Finding it algorithmically requires derivate information.
    \end{frame}


    \section{Derivatives and Gradients}
    \begin{frame}{The derivative}
      \begin{align}
        \dfrac{d f(x)}{dx} = \lim_{h \rightarrow 0} \dfrac{f(x + h) - f(x)}{h}
      \end{align}
      \begin{figure}
        \includestandalone[scale=.7]{./figures/parabola_derivative}
      \end{figure}
      \note{TODO}
    \end{frame}

    \begin{frame}{Derivation of the parabola derivative}
      \begin{align}
          \lim_{h \rightarrow 0} \dfrac{(x + h)^2 - x^2}{h} 
          & \Leftrightarrow \lim_{h \rightarrow 0} \dfrac{x^2 + 2xh + h^2 - x^2}{h}   \\
          & \Leftrightarrow \lim_{h \rightarrow 0} \dfrac{2xh + h^2}{h}  \\
          & \Leftrightarrow \lim_{h \rightarrow 0} \dfrac{h (2x + h)}{h}  \\
          & \Leftrightarrow \lim_{h \rightarrow 0} 2x + h \\
          & \Leftrightarrow 2x
      \end{align}
    \end{frame}

    \begin{frame}{Steepest descent on the parabola}
      \begin{align}
        x_n = x_{n-1} - \alpha \cdot \dfrac{d f}{dx}
      \end{align}
      Working with $x_0 = 5$ and $\alpha = 0.1$ for $25$ steps leads to: 
      \begin{figure}
        \includestandalone[scale=0.7]{./figures/parabola_opt}
      \end{figure}
      \note{TODO: Add plot of the derivative.}
    \end{frame}

    \begin{frame}{Multidimensional problems}
      \begin{align}
        f(x, y) = (a - x)^2 + b(y - x^2)^2
      \end{align}
      \begin{figure}
        \includestandalone[scale=0.7]{./figures/rosenbrock}
        \caption{Rosenbrock function with a=1 and b=100 .}
      \end{figure}
    \end{frame}

    \begin{frame}{The gradient}
      \begin{align}
        \nabla f = \begin{pmatrix}
          \frac{\partial f}{\partial x_1} \\
          \frac{\partial f}{\partial x_2} \\
          \vdots \\
          \frac{\partial f}{\partial x_n}
        \end{pmatrix}
      \end{align}
      \note{TODO}
    \end{frame}

    \begin{frame}{Rosenbrock gradient}
      \begin{align}
        \nabla f(x, y) = \begin{pmatrix}
          -2a + 2x - 4byx + 4bx^3 \\
          2by - 2bx^2 \\
        \end{pmatrix}
      \end{align}
      \note{TODO}
    \end{frame}

    \begin{frame}{Gradient descent}
      $x_0 = [0.1, 3.]$,
      $\alpha = 0.01 $
      \begin{align}
        x_n = x_{n-1} - \alpha \cdot \nabla f(x)
      \end{align}
    \end{frame}

    \begin{frame}{Gradient descent on the Rosenbrock function}
      \centering
      \movie[width=5.45cm,height=4cm,poster]{Rosenbrock Optimization}{rosenbrock_momentum_0.0.mp4}
    \end{frame}

    \begin{frame}{Gradient descent with momentum}
      \centering
      \movie[width=5.45cm,height=4cm,poster]{Rosenbrock Optimization}{rosenbrock_momentum_0.8.mp4}
    \end{frame}

    \section{Optimization}
    \begin{frame}{Second order optimization}
        TODO
    \end{frame}

\end{document}
